---
title: "Session 1"
format: live-html
filters: 
  - webr
  - naquiz
engine: knitr
fig-cap-location: top
theme: lux
webr:
  render-df: default
  packages: ["tibble", "ggplot2", "dplyr", "purrr"]
---

## Measures of Central Tendency

A measure of central tendency is a single value that attempts to describe a set of data by identifying the central position within that set of data. The mean and median are the most commonly used measures of central tendency for numerical data. \## The Mean The mean, one of the simplest concepts in statistics, is a quantity representing the "center" of a collection of numbers and is intermediate to the extreme values of the set of numbers. The mean can be calculated by taking the sum of the values in a series and dividing by the number of values (see @eq-mean). However, the mean can also be thought of as a quantity that minimizes the distance from all values in set of numbers. Although thinking about the mean in this way may seem unnecessarily indirect (perhaps even silly), we will use this perspective to demonstrate some coding concepts that we need to know to facilitate calculating more complex things down the line. In addition, thinking about the mean as a value that minimizes the distance from all numbers in a series of numbers taps into the concept of *sum of squares*, something we will talk about more in future sessions.

<p style="margin:35px;">

</p>

$$\Large \bar{x}= \frac{1}{n} \sum_{i=1}^{n}(x_i)$$ {#eq-mean}

<p style="margin:50px;">

</p>

## Searching for the mean

@fig-plot shows a schematic of how we might think about searching for the mean by introducing guesses (g~1~, g~2~, g~3~). The best guess will be the one that minimizes the distance from the guessed value and the observed numbers (y~1~, y~2~, y~3~). But how should be summarize the distances across observations?

```{r}
#| echo: FALSE 
#| message: FALSE
#| warning: FALSE
#| fig-align: "center"
#| fig-width: 4.5
#| fig-height: 2.5
#| out-width: "80%"
#| label: fig-plot
#| fig-cap: Schematic representing the process of how to search for the mean
library(tidyverse)
d <- tibble(guess = rep(c("GUESS 1", "GUESS 2", "GUESS 3"), 
                        each = 3),
            y = rep(c(1, 3, 4), 3))
d %>% 
  ggplot(aes(x = guess, 
             y = y)) +
  geom_point(alpha = 0) +
  annotate("point", x = c(0.85, 1.85, 2.85), 
           y = 3.5, 
           shape = 21, 
           size = 2.5, 
           stroke = 1) +
  annotate("text", x = c(0.75, 1.75, 2.75), 
           y = 3.5, 
           label = "y[1]", 
           size = 3, 
           parse = TRUE, 
           family = "Nunito Sans") +
  annotate("point", x = c(1.15, 2.15, 3.15), 
           y = 2.8, 
           shape = 21, 
           size = 2.5, 
           stroke = 1) +
  annotate("text", x = c(1.05, 2.05, 3.05), 
           y = 2.8, 
           label = "y[3]", 
           size = 3, 
           parse = TRUE, 
           family = "Nunito Sans") +
  annotate("point", x = c(1, 2, 3), 
           y = 1.4, 
           shape = 21, 
           size = 2.5, 
           stroke = 1) +
  annotate("text", x = c(0.9, 1.9, 2.9), 
           y = 1.4, 
           label = "y[2]", 
           size = 3, 
           parse = TRUE, 
           family = "Nunito Sans") +
  annotate("segment", 
           y = 3.5, 
           yend = 2.6, 
           x = 0.85, 
           linetype = "dashed") +
  annotate("segment", 
           y = 2.8, 
           yend = 2.6, 
           x = 1.15, 
           linetype = "dashed") +
  annotate("segment", 
           y = 1.4, 
           yend = 2.6, 
           x = 1, 
           linetype = "dashed") +
  annotate("segment", 
           x = 0.8,
           xend = 1.2, 
           y = 2.6, 
           color = "darkblue", 
           linewidth = 1) +
  annotate("text", 
           y = 2.6, 
           x = 1.28,
           label = "g[1]",
           size = 3,
           parse = TRUE, 
           family = "Nunito Sans") +
  annotate("segment", 
           y = 3.5, 
           yend = 2.4, 
           x = 1.85, 
           linetype = "dashed") +
  annotate("segment", 
           y = 2.8, 
           yend = 2.4, 
           x = 2.15, 
           linetype = "dashed") +
  annotate("segment", 
           y = 1.4, 
           yend = 2.4, 
           x = 2, 
           linetype = "dashed") +
  annotate("segment", 
           x = 1.8,
           xend = 2.2, 
           y = 2.4, 
           color = "darkorange", 
           linewidth = 1) +
  annotate("text", 
           y = 2.4, 
           x = 2.28,
           label = "g[2]",
           size = 3,
           parse = TRUE, 
           family = "Nunito Sans") +
  annotate("segment", 
           y = 3.5, 
           yend = 2.2, 
           x = 2.85, 
           linetype = "dashed") +
  annotate("segment", 
           y = 2.8, 
           yend = 2.2, 
           x = 3.15, 
           linetype = "dashed") +
  annotate("segment", 
           y = 1.4, 
           yend = 2.2, 
           x = 3, 
           linetype = "dashed") +
  annotate("segment", 
           x = 2.8,
           xend = 3.2, 
           y = 2.2, 
           color = "darkred", 
           linewidth = 1) +
  annotate("text", 
           y = 2.2, 
           x = 3.28,
           label = "g[3]",
           size = 3,
           parse = TRUE, 
           family = "Nunito Sans") +
  labs(y = NULL, 
       x = NULL) +
  theme_minimal() +
  theme(panel.grid.minor = element_blank(), 
        text = element_text(size = 10, family = "Nunito Sans", face = "plain"))
```

## Sum of Squares

In statistics, the sum of squares is a measure of variability or dispersion within a set of data. It represents the sum of the squared differences between each data point and a central value, typically the mean (or in our case each guess). It is a fundamental concept in regression analysis and ANOVA, helping to assess how well a model fits the data. @eq-ssqg shows how the concept of sum of squares can be adopted to our *guess the mean* scenario.\

<p style="margin:30px;">

</p>

$$\Large SSQ_{g_1}= \sum_{i=1}^{n}(y_i - g_1)^2$$ {#eq-ssqg}

<p style="margin:35px;">

</p>

If we translate @eq-ssqg to plain English, it tells us that in order to calculate SSQ~g1~, we should calculate the difference between our guess (g~1~) and our y-values (y~1~ - g~1~, y~2~ - g~1~, y~3~ - g~1~), then square the differences and finally sum up the squared differences. Why do square the differences before summing, you might ask? In the context of statistical analysis, the squaring operation in the *sum of squares* is used because: \* By squaring, all deviations are treated as positive values, preventing cancellation effects when calculating the total dispersion or variation. For the purpose of what we are doing today, this is the most important reason why we square the differences. \* Squaring also has the effect of giving more weight to larger deviations. This means that data points further away from the mean (or guess in our case) will have a greater impact on the overall sum of squares, which can be desirable in many statistical applications. \* From a mathematical perspective, squaring is often preferred because it leads to simpler calculations, especially when dealing with derivatives in optimization problems. We may or may not talk more about this in the future.

```{webr-r}
#| context: output
mean_guess_sm <- tibble(y = rep(c(3.5, 2.8, 1.4), 3),
                        guess = rep(c("g1", "g2", "g3"), each = 3),
                   guess_value = rep(c(2.6, 2.4, 2.2), each = 3))
```

## Turn math into code

Through out this training, we will use our R skills to help us unpack the mathematics underlying statistical methods. We will start by writing code that will for each of our guesses (g~1~, g~2~, g~3~) in @fig-plot calculate the *sum of squares* as described in @eq-ssqg. A small data frame (tibble) `mean_guess_sm`, containing the values in @fig-plot, has already been generated. Use the interactive code block below to take a look at these data.

<p style="margin:30px;">

</p>

```{webr-r}
mean_guess_sm
```

<p style="margin:35px;">

</p>

Great, we have some data and it seems to correspond well with @fig-plot. Now we are going to go through the different parts of @eq-ssqg and turn them into R code. First, let's add a new column to `mean_guess_sm` containing the difference between the y-values and the guess values. To do this we will use the `mutate()` function from the `dplyr` package (part of the tidyverse). We will call the new variable `diff`.

<p style="margin:30px;">

</p>

```{webr-r}
mean_guess_sm %>% 
  mutate(diff = y - guess_value)
```

<p style="margin:35px;">

</p>

We see that calculating the difference results in both positive and negative values. If not dealt with, this could cause cancellation effects issues (as mentioned above). This is (one of the reasons, the most important reason) why we next square the differences.

<p style="margin:30px;">

</p>

```{webr-r}
mean_guess_sm %>% 
  mutate(diff = y - guess_value, 
         diff_sq = diff^2)
```

<p style="margin:35px;">

</p>

As expected, all squared differences are positive. If we revisit @eq-ssqg, we see that the only thing left for us to do is sum up (that is what $\sum$ means) the squared differences. But wait a minute, @eq-ssqg shows that the process of calculating the sum of squares is done for each guess separately (hence the naming SSQ~g1~). So far this has not been an issue because the guess values and y-values were repeated in the data in a way so that it would be easy to calculate the squared differences. But when summing up the squared differences, we need to make sure that we only sum the three values beloning to each guess group. We will use the `group_by()` function to accomplish this. After grouping we will use the `summarize()` function to calculate the sum.

<p style="margin:30px;">

</p>

```{webr-r}
#| warning: false
mean_guess_sm %>% 
  mutate(diff = y - guess_value, 
         diff_sq = diff^2) %>% 
  group_by(guess, guess_value) %>% 
  summarize(ssq = sum(diff_sq))
```

<p style="margin:35px;">

</p>

We see that g~1~ = 2.6 is the best guess (smallest `ssq` value) out of our three guesses. We of course have no reason to believe that this is a particularly good guess, and in order to get closer to the true mean we would have to try a sequence of many guesses for which values differ by small increments. Let's try this with a bigger data set (more y-values).\
\## More data, More guesses! In the object `y_values_250` I have stored 250 observations sampled from a normal distribution. A histogram of these values is shown below.

```{webr-r}
#| context: output
set.seed(14)
y_values_250 <- tibble(values = rnorm(250, mean = 2.5))
```

```{r}
#| echo: false
#| message: FALSE
#| warning: FALSE
#| fig-align: "center"
#| fig-width: 4.5
#| fig-height: 2.5
#| out-width: "80%"
#| label: fig-hist
#| fig-cap: Histogram of 250 observation sampled from a normal distribution
set.seed(14)
y_values_250 <- tibble(values = rnorm(250, mean = 2.5))
y_values_250 %>% 
  ggplot(aes(x = values)) +
  geom_histogram(bins = 15,
                 fill = "grey75", 
                 color = "black") +
  labs(x = NULL) +
  theme_minimal() +
  theme(panel.grid.minor = element_blank())
```

We are now going to repeat the process described above; searching for the mean by trying many guesses and see which one results in the smallest sum of squares. First, let's take a look at the data so that we can start to get an idea of what we need to do next.

<p style="margin:30px;">

</p>

```{webr-r}
y_values_250
```

<p style="margin:35px;">

</p>

What we have is a tibble with one column named `values` storing the 250 observations shown in @fig-hist. Next, we will use the same idea as described above to search for the mean. Taking a look at @fig-hist and acknowledging the fact that the mean is positioned in the center of a normal distribution, it seems reasonable that the mean will be somewhere between 2.4 and 2.6. Therefore, we will go through 1,000 guesses this time, evenly spaces between these two values. Note that there are two tabs with output; one interactive code chunk, but since these code chunks cannot at this point output interactive plots, a plotly plot can be found in the second tab. We will use the inteactive plotly plot to hover over the curve in order to find the guess that corresponds to the smallest ssq value.

<p style="margin:30px;">

</p>

::: panel-tabset
## Interactive code

```{webr-r}
#| fig-height: 4
add_guess <- function(guess){
  y_values_250 %>% 
    mutate(guess = guess)
}
res <- map(seq(2.4, 2.6, length.out = 1000), \(i) add_guess(i))
res_df <- res %>% 
  map(. %>% 
        mutate(diff_sq = (values - guess)^2) %>% 
        summarise(ssq = sum(diff_sq), 
                  guess = first(guess))) %>% 
  list_rbind()
res_df %>% 
  ggplot(aes(x = guess, 
             y = ssq)) +
  geom_point() +
  theme_minimal()
```

## Plotly

```{r}
#| fig-height: 4
add_guess <- function(guess){
  y_values_250 %>% 
    mutate(guess = guess)
}
res <- map(seq(2.4, 2.6, length.out = 1000), \(i) add_guess(i))
res_df <- res %>% 
  map(. %>% 
        mutate(diff_sq = (values - guess)^2) %>% 
        summarise(ssq = sum(diff_sq), 
                  guess = first(guess))) %>% 
  list_rbind()
p1 <- res_df %>% 
  ggplot(aes(x = guess, 
             y = ssq)) +
  geom_point() +
  theme_minimal()
plotly::ggplotly(p1)
```
:::

<p style="margin:35px;">

</p>

And now for grand reveal! Lets check if the value found in the plotly output corresponds to the actual mean.

<p style="margin:30px;">

</p>

```{webr-r}
y_values_250 %>% 
  summarise(mean(values))
```

<p style="margin:35px;">

</p>

## For you to think about

Earlier we explained that when calculating the sum of squares we square the distances to make sure that all values are positive and hence avoid issues with cancellation effects. But why not use the absolute value of the distance instead, you might ask? Use the interactive windows above and see if the mean can also be found if taking the absolute value instead of squaring. Then answer the following question: :::::{.question} Using the absolute instead of the squared difference in this case calculates what? ::::{.choices} :::{.choice} Mean :::\
:::{.choice} Mode ::: :::{.choice} Median ::: :::{.choice .correct-choice} MAD (Median Absolute Deviation)\
::: :::: :::::
